# -*- coding: utf-8 -*-
"""LinearRegressionWithGradientDescent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wYrrqAnsgHssiWzZrn2k44Y-v-zOM1c8
"""

/**
 * @author Cagri Toraman
 * cagritoraman@gmail.com
 */
 
import numpy as np
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

def gradient_descent(X, y, lr, epoch):
    
    m, b = 0.5, 0.5 # parameters
    log, mse = [], [] # lists to store learning process
    N = len(X) # number of samples
    
    for _ in range(epoch):
        print("\n",_)    
        f = y - (m*X + b)
        print("f: ",f)
        # Updating m and b
        print("m: ",m,"-",lr,"*",(-2 * X.dot(f).sum() / N))
        m -= lr * (-2 * X.dot(f).sum() / N)
        b -= lr * (-2 * f.sum() / N)
        
        
        log.append((m, b))
        mse.append(mean_squared_error(y, (m*X + b)))        
        # print((m,b))
        print("mean_squared_error: ",mean_squared_error(y, (m*X + b)))
        # plt.plot(m*X+b, label=_)
        # plt.legend()
    
    return m, b, log, mse

X = np.array([0.2,0.5,0.3,0.1,0.8])
y = np.array([0.1,0.2,0.3,0.4,0.5])
print("X: ",X,", y: ",y)
print(len(X),", ",len(y))
m, b, log, mse = gradient_descent(X,y,0.05,20)
print("Final weights: m=",m,", b=",b)
# plt.scatter(X,y)
# plt.plot(X,(m*X+b))
plt.plot(mse)

